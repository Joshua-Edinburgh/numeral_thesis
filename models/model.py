#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jun 18 21:28:06 2019

@author: xiayezi
"""
import sys
sys.path.append("..")

from utils.conf import *
from torch.distributions.categorical import Categorical
from torch.distributions.one_hot_categorical import OneHotCategorical
from torch.distributions.relaxed_categorical import RelaxedOneHotCategorical



# ======== Test for data generation =========
data_batch = np.random.randint(0, 10**ATTRI_SIZE-1, (BATCH_SIZE, 1))


class DataEncoderMLP(nn.Module):
    '''
        Input data is a [N_B,1] tensor, each number has ATTRI_SIZE attributes,
        each attribute has NUM_SYSTEM values. Output shape [N_B, 1, hidden]
        Test:
            t = DataEncoderMLP()
            test = t.gen_embedding(data_batch)  
            test_forward = t.forward(data_batch)
    '''
    def __init__(self, hidden_size=HIDDEN_SIZE):
        super(DataEncoderMLP, self).__init__()
        self.hidden_size = hidden_size
        self.emb_size = ATTRI_SIZE*NUM_SYSTEM
             
        self.lin = nn.Linear(self.emb_size, hidden_size)     
        
    def gen_embedding(self, data_batch):
        '''
            Change [N_B,1] data to [NB,1,ATTRI_SIZE*NUM_SYSTEM] tensor
        '''
        data_embeddings = np.zeros((BATCH_SIZE, 1, self.emb_size))
        for b in range(BATCH_SIZE):
            for i in range(ATTRI_SIZE):
                tmp = int(np.mod(data_batch[b] / (NUM_SYSTEM**i), NUM_SYSTEM))
                index = int(tmp + NUM_SYSTEM*i)
                data_embeddings[b, 0, index] = 1.                
        return torch.tensor(data_embeddings).float().to(DEVICE)
    
    def forward(self, data_batch):
        data_embeddings = self.gen_embedding(data_batch)
        h = self.lin(data_embeddings)        
        return F.softmax(h)


class MsgGenGRU(nn.Module):
    '''
        Give the hidden generated by DataEncoderMLP, [N_B, 1, hidden], use GRU
        to generate message, [N_B, Max_len, hidden], and the mask having same shape
        The input size is voc_size, initialized as all zeros.
    '''
    def __init__(self, voc_size = MSG_VOCSIZE, hidden_size=HIDDEN_SIZE):
        super(DataEncoderMLP, self).__init__()
        self.hidden_size = hidden_size
        self.input_size = voc_size
        
        self.gru = nn.GRU(input_size=voc_size, hidden_size=hidden_size, batch_first=True)
        self.init_input = nn.Parameter(torch.zeros(1, self.input_size, device=DEVICE))
        
    def forward(self, h_0):
        batch_size = h_0.size(0)
        decoder_input = self.init_input.expand(batch_size, -1)
        decoder_hidden = h_0
        message = []
        mask = []
        
        _mask = torch.ones((1, batch_size), device=DEVICE)
        log_probs = 0.        

        for _ in range(MSG_MAX_LEN):
            mask.append(_mask)
            decoder_hidden = self.gru(decoder_input, decoder_hidden)
            probs = F.softmax(self.out(decoder_hidden), dim=1)        


class PredictorMLP(nn.Module):
    '''
        For listener to decide which candidate is the target. We concatenate 
        hidden states generated by listener decoder and dataencoder, then pass
        an MLP and get a distribution.
    '''
    def __init__(self, hidden_size=HIDDEN_SIZE):
        super(PredictorMLP, self).__init__()
        self.hidden_size = hidden_size
        self.emb_size = ATTRI_SIZE*NUM_SYSTEM
                
        self.lin1 = nn.Sequential(
            nn.Linear(self.emb_size+hidden_size, hidden_size),  # We concatenate two vectors
            nn.ReLU(),
        )
        self.lin2 = nn.Linear(hidden_size, 1)
